---
title: "Merging Files"
author: "Pete Way"
date: "June 21, 2019"
output: pdf_document
---


```{r}
##Merging Weather Files (That have similar titles)

library(stringr)

newhour = read.csv("MissingHourWeather S1 Full.csv")
for (i in c(2,4)){
  title = c("MissingHourWeather S",i," Full.csv")
  title = str_c(title, collapse = "")
  print(title)
  weather = read.csv(title)
  newhour = rbind(newhour, weather)
  length(new)
}
write.csv(newhour, file = "ALLMissingWeather.csv")
```


```{r}
##Raising memory limits when you need more flexibility. 
memory.limit()
memory.limit(size=56000)
```

```{r}
library(stringr)
bigfile = read.csv("/Users/pete/Downloads/All2019Weather.csv")
data = bigfile[ which(bigfile$ORIG_FID < 100 & bigfile$ORIG_FID >= 0), ]
write.csv(data, file = "/Users/pete/Downloads/2019Weather_0_100.csv")
##Splitting a big data set into smaller chunks for easier importing
for (i in seq(0,800,by=100)){
  j = i+100
  if (i == 800){
    j=906
  }
data = bigfile[ which(bigfile$ORIG_FID < j & bigfile$ORIG_FID >= i), ]
title = c("Weather2019_",i,"_",j,"_Full.csv")
title = str_c(title, collapse = "")
print(title)
write.csv(data, file = title)
}

```


```{r}
##Merging Accident and Negative files, and ordering by date and time. 
file = read.csv("/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/Grid Oriented Layout Test Files/NegativeSampling/NS True Master List Matched.csv")
accidents = read.csv("/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/Grid Oriented Layout Test Files/NegativeSampling/GOD 2017+2018 Accidents.csv")

colnames(accidents)
colnames(file)

colnames(file)[colnames(file)=="Center_Long"] <- "Longitude"
colnames(file)[colnames(file)=="Center_Lat"] <- "Latitude"

drops <- c('Temperature','Temp_Max','Temp_Min','Daily_Relative_Temp','Monthly_Relative_Temp','X','Weekday' )
accidents = accidents[ , !(names(accidents) %in% drops)]

todrop = "Month"
accidents = accidents[ , !(names(accidents) %in% todrop)]
file = file[ , !(names(file) %in% todrop)]

colnames(accidents)
for (i in seq(1,length(colnames(accidents)))){
  cat(colnames(accidents)[i], colnames(file)[i], "\n")
}

complete = rbind(accidents, file)
complete[with(complete, -order("Date","Time")),]
write.csv(complete, file = "/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/Grid Oriented Layout Test Files/NegativeSampling/True Randoms/True Random Data 2017+2018.csv")
```


```{r}
##Finding the division of data in training and testing sets
training = read.csv("/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/Grid Oriented Layout Test Files/NegativeSampling/GridFixed/NoShuffle_X_Train.csv")
testing = read.csv("/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/Grid Oriented Layout Test Files/NegativeSampling/GridFixed/NoShuffle_X_Test.csv")

tabTrain = table(training$Accident)
tabTest = table(testing$Accident)

tabTrain
tabTest
```

```{r}

########Taking the full data set and splitting into training and testing by the First of July 2018
fullgrid = read.csv("/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/Grid Oriented Layout Test Files/NegativeSampling/Full GridFix/Full GridFix Data 2017+2018.csv")

##Losing all data that is empty
for (i in seq(1,length(colnames(fullgrid)))){
  fullgrid = fullgrid[which(!is.na(fullgrid[i])), ]
 print(length(fullgrid$Accident))
}

##To Split by a date, select the date, and then run these three lines!
fullgrid$Date <- as.Date(fullgrid$Date)
test = with(fullgrid, fullgrid[(Date >= "2018-06-30" ), ])
train = with(fullgrid, fullgrid[(Date <= "2018-07-01" ), ])
test = test[with(test, -order("Date","Time")),]
train = train[with(train, -order("Date","Time")),]

tokeep = c("Accident", "Hour", "DayFrame", "WeekDay", "WeekEnd", "Precipitation_Intensity", "Clear", "Cloudy", "Rain", "Fog", "Snow", "RainBefore", "Grid_Block", "Grid_Col", "Grid_Row", "Highway", "Land_Use_Mode", "Road_Count")
test = test[ , (names(test) %in% tokeep) ]
train = train[ , (names(train) %in% tokeep) ]

write.csv(train, file = "/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Graphs & Images/ResultsFromFullGridFixTesting/Second Test - No Shuffle/Full Grid Fix Non-Shuffle Training 1.csv")
write.csv(test, file = "/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Graphs & Images/ResultsFromFullGridFixTesting/Second Test - No Shuffle/Full Grid Fix Non-Shuffle Testing 1.csv")

colMeans(is.na(testing))
colMeans(is.na(training))
```

```{r}
##Combining Blocks from multiple files 

/Users/pete/Google Drive/Weather2019_100_200_Full.csv

blocks = read.csv("/Users/pete/Downloads/2019 Weather Blocks 1.csv")
library(stringr)
for (i in seq(2:8)){
  title = c("/Users/pete/Downloads/2019 Weather Blocks ",i,".csv")
  title = str_c(title, collapse = "")
  print(title)
  weather19 = read.csv(title)
  blocks = rbind(blocks, weather19)
}

length(unique(blocks$ORIG_FID))
unique(blocks$Date)
tab = table(blocks$ORIG_FID, blocks$Date)
tab
# datesbyblock = as.data.frame.matrix(tab) 
# write.csv(datesbyblock, file = "/Users/pete/Downloads/DatesbyBlocks.csv")

blocks = distinct(blocks, ORIG_FID,time, .keep_all= TRUE)
write.csv(blocks, file = "/Users/pete/Downloads/All2019Weather.csv")
```

```{r}
data = read.csv("/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Graphs & Images/ResultsFromFullGridFixTesting/Second Test - No Shuffle/Full Grid Fix Non-Shuffle Testing 1.csv")
plot(fullgrid$Accident[0:10000])
tab = table(fullgrid$Accident)
tab
```

```{r}
accidents19 = read.csv("/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/2019 Data/2019_Data.csv")
weekdaydata19 = with(accidents19, accidents19[(WeekDay == 1 ), ])
weekenddata19 = with(accidents19, accidents19[(WeekEnd == 1 ), ])
# blocks = sort(unique(accidents19$Grid_Block))
# length(blocks)
framebyblockWeekday19 = table(weekdaydata19$Grid_Block, weekdaydata19$DayFrame)
framebyblockWeekend19 = table(weekenddata19$Grid_Block, weekenddata19$DayFrame)
accidentsbyframeblockWeekday19 = as.data.frame.matrix(framebyblockWeekday19) 
accidentsbyframeblockWeekend19 = as.data.frame.matrix(framebyblockWeekend19) 

hourbyblockWeekend19 = table(weekenddata19$Grid_Block, weekenddata19$Hour)
hourbyblockWeekday19 = table(weekdaydata19$Grid_Block, weekdaydata19$Hour)
accidentsbyhourblockWeekday19 = as.data.frame.matrix(hourbyblockWeekday19) 
accidentsbyhourblockWeekend19 = as.data.frame.matrix(hourbyblockWeekend19) 


write.csv(accidentsbyframeblockWeekday19, file = "/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/7 Way Negative Sampling/AccidentCountsbyDayFrame&BlockWeekday2019.csv")
write.csv(accidentsbyframeblockWeekend19, file = "/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/7 Way Negative Sampling/AccidentCountsbyDayFrame&BlockWeekend2019.csv")

write.csv(accidentsbyhourblockWeekday19, file = "/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/7 Way Negative Sampling/AccidentCountsbyHour&BlockWeekday2019.csv")
write.csv(accidentsbyhourblockWeekend19, file = "/Users/pete/Documents/GitHub/SCAL_USIgnite-911/Excel & CSV Sheets/7 Way Negative Sampling/AccidentCountsbyHour&BlockWeekend2019.csv")
```

```{r}
temporalnegatives = read.csv("../Excel & CSV Sheets/Systematic Negative Sampling/2019_Systematic_Negatives_Temporal_Shift.csv")

unique(temporalnegatives$Grid_Block)
empty = with(temporalnegatives, temporalnegatives[is.na(Grid_Block), ])

weatherdata = read.csv("/Users/pete/Downloads/All2019Weather.csv")
unique(weatherdata$Date)
tab = table(weatherdata$ORIG_FID, weatherdata$Date)
tab = as.data.frame.matrix(tab) 


colnames(weatherdata)
unique(weatherdata$ORIG_FID)
weatherdata$Date[1]
drop = c("X.1", "X", "Unnamed..0")
weatherdata = weatherdata[ , !(names(weatherdata) %in% drop) ]
weatherdata$Date[0:5]
typeof(weatherdata$Date[0])
temporalnegatives$Date[0:5]
typeof(temporalnegatives$Date[0])

brokeat = with(weatherdata, weatherdata[(ORIG_FID == 249 & timereadable == 0 & Date == '2019-01-05'), ])
```



